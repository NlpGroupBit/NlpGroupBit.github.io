+++
date = "2024-07-20"
short_text = ""
title = "Several papers from the lab accepted by ACL 2024"
[[authors]]
    is_member = true
    id = "member1"
+++


In July 2024, 2 papers from the academic group were accepted at the ACL 2024 conference. ACL is the Association for Computational Linguistics, which is the most influential academic organisation in the field of computational linguistics, and is the most important international conference in the field of computational linguistics, and is the only Class A conference in computational linguistics recommended by CCF.

The brief description of the accepted papers is given below:
### Word Matters: What Influences Domain Adaptation in Summarization?
Yinghao Li, Siyu Miao, Heyan Huang, Yang Gao
>Abstract: Domain adaptation aims to enable Large Language Models (LLMs) to generalize domain datasets unseen effectively during the training phase. However, factors such as the size of the model parameters and the scale of training data are general influencers and do not reflect the nuances of domain adaptation performance. This paper investigates the fine-grained factors affecting domain adaptation performance, analyzing the specific impact of `words' in training data on summarization tasks. We propose quantifying dataset learning difficulty as the learning difficulty of generative summarization, which is determined by two indicators: word-based compression rate and abstraction level. Our experiments conclude that, when considering dataset learning difficulty, the cross-domain overlap and the performance gain in summarization tasks exhibit an approximate linear relationship, which is not directly related to the number of words. Based on this finding, predicting a model's performance on unknown domain datasets is possible without undergoing training.

### Fundamental Capabilities of Large Language Models and their Applications in Domain Scenarios: A Survey
Jiawei Li, Yizhe Yang, Yu Bai, Xiaofeng Zhou, Yinghao Li, Huashan Sun, Yuhang Liu, Xingpeng Si, Yuhao Ye, Yixiao Wu, Yiguan Lin, Bin Xu, Bowen Ren, Chong Feng, Heyan Huang, Yang Gao
>Abstract: Large Language Models (LLMs) demonstrate significant value in domain-specific applications, benefiting from their fundamental capabilities. Nevertheless, it is still unclear which fundamental capabilities contribute to success in specific domains. Moreover, the existing benchmark-based evaluation cannot effectively reflect the performance of real-world applications. In this survey, we review recent advances of LLMs in domain applications, aiming to summarize the fundamental capabilities and their collaboration. Furthermore, we establish connections between fundamental capabilities and specific domains, evaluating the varying importance of different capabilities. Based on our findings, we propose a reliable strategy for domains to choose more robust backbone LLMs for real-world applications.